{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script essentially uses the pre-trained SVM model to perform face detection on a test image using a sliding window approach at various scales. Detected bounding box coordinates are shown on the test image, providing visual representation of detected faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Importing Required Modules and Libraries:**\n",
    "\n",
    "* Import functions from a custom utility module.\n",
    "* Import the joblib library for loading the pre-trained SVM model.\n",
    "* Import necessary classes from scikit-learn for creating custom transformers.\n",
    "* Import the Pipeline class for creating a processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loading Pre-trained SVM Model:**\n",
    "\n",
    "* Load a pre-trained SVM model that includes hard negatives. This model was trained using the script in _train_model.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_with_hard_negatives=joblib.load('svm_with_hard_negatives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Creating a Custom Transformer for Object Detection:**\n",
    "\n",
    "* ObjectDetectionTransformer class is defined, inheriting from BaseEstimator and TransformerMixin.\n",
    "* The constructor (__init__) initializes the transformer with attributes including the pre-trained SVM model, window sizes, step size, downscale factor, and confidence threshold.\n",
    "* detect_objects method takes an image and a window size, and performs object detection using a sliding window approach at different scales. It calculates HOG features for each window, predicts using the SVM model, and accumulates detections based on confidence scores.\n",
    "* transform method takes input data (images) and returns detected bounding box coordinates after non-maximum suppression (see utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, svm_with_hard_negatives, winSizes, stepSize, downscale, threshold):\n",
    "        self.svm_with_hard_negatives = svm_with_hard_negatives\n",
    "        self.winSizes = winSizes\n",
    "        self.stepSize = stepSize\n",
    "        self.downscale = downscale\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect_objects(self, image, winSize):\n",
    "        detections = []\n",
    "        for scale in np.linspace(1.0, self.downscale, 5)[::-1]:\n",
    "            resized_image = cv2.resize(image, (int(image.shape[1] / scale), int(image.shape[0] / scale)))\n",
    "            for (x, y, window) in sliding_window(resized_image, self.stepSize, winSize):\n",
    "                if window.shape[0] != winSize[1] or window.shape[1] != winSize[0]:\n",
    "                    continue\n",
    "                features = get_hog_features(window)  \n",
    "                confidence = self.svm_with_hard_negatives.decision_function([features])[0]\n",
    "                if confidence >= self.threshold:\n",
    "                    x = int(x * scale)\n",
    "                    y = int(y * scale)\n",
    "                    w = int(winSize[0] * scale)\n",
    "                    h = int(winSize[1] * scale)\n",
    "                    detections.append((x, y, x + w, y + h))\n",
    "        return detections\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        all_detections = []\n",
    "        for winSize in self.winSizes:\n",
    "            detections = self.detect_objects(X, winSize)\n",
    "            all_detections.extend(detections)\n",
    "        nms_detections = non_max_suppression(np.array(all_detections), overlap_threshold=0.2)\n",
    "        return nms_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Defining Parameters for Object Detection:**\n",
    "\n",
    "* winSizes: A list of window sizes (bounding box dimensions) for object detection.\n",
    "* stepSize: Step size for sliding window approach.\n",
    "* downscale: A factor for resizing the image to detect objects at different scales.\n",
    "* threshold: Confidence threshold for deciding if an object is detected.\n",
    "\n",
    "**5. Creating an Object Detection Pipeline:**\n",
    "\n",
    "object_detection_pipeline is defined as a scikit-learn Pipeline with a single step using the ObjectDetectionTransformer. This pipeline encapsulates the object detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSizes = [(128, 128)]\n",
    "stepSize = 10\n",
    "downscale = 1.5\n",
    "threshold = 0.7\n",
    "\n",
    "object_detection_pipeline = Pipeline([\n",
    "    ('object_detection', ObjectDetectionTransformer(svm_with_hard_negatives, winSizes, stepSize, downscale, threshold))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Loading and Preparing a Test Image:**\n",
    "\n",
    "* test_image_path: Path to the test image file.\n",
    "* test_image: Load the test image using OpenCV.\n",
    "\n",
    "**7. Performing Object Detection:**\n",
    "\n",
    "nms_detections: Perform object detection using the defined pipeline on the test image.\n",
    "If objects are detected, the bounding box coordinates are stored in the nms_detections array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"test/thelastofus.jpg\"\n",
    "test_image = cv2.imread(test_image_path)\n",
    "\n",
    "nms_detections = object_detection_pipeline.transform(test_image)\n",
    "\n",
    "if len(nms_detections) > 0:\n",
    "    bounding_box_list = nms_detections.tolist()\n",
    "    print(bounding_box_list)\n",
    "else:\n",
    "    print(\"NO FACES DETECTED IN THE IMAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Displaying Results:**\n",
    "\n",
    "* If no faces are detected, the script prints \"NO FACES DETECTED IN THE IMAGE\".\n",
    "* If faces are detected:\n",
    "  * The script creates a copy of the test image (result_image).\n",
    "  * It draws green rectangles around the detected bounding boxes on result_image.\n",
    "  * The modified image is displayed using OpenCV, showing the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(nms_detections) == 0:\n",
    "    print(\"NO FACES DETECTED IN THE IMAGE\")\n",
    "else:\n",
    "    result_image = test_image.copy()\n",
    "    for (x1, y1, x2, y2) in nms_detections:\n",
    "        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Object Detection Results\", result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
